"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
"""

import json
import pandas as pd
from collections import Counter

def check_duplicates():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    print("üîç –ü–†–û–í–ï–†–ö–ê –î–£–ë–õ–ò–ö–ê–¢–û–í URL –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô")
    print("="*50)
    
    # –ò—â–µ–º JSON —Ñ–∞–π–ª —Å URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    import glob
    json_files = glob.glob("image_urls_*.json")
    
    if not json_files:
        print("‚ùå JSON —Ñ–∞–π–ª—ã —Å URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return None
    
    # –ë–µ—Ä–µ–º —Å–∞–º—ã–π –Ω–æ–≤—ã–π —Ñ–∞–π–ª
    latest_file = max(json_files)
    print(f"üìÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∞–π–ª: {latest_file}")
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        with open(latest_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π –æ–± –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ URL
        all_urls = [item['image_url'] for item in data]
        print(f"üîó –í—Å–µ–≥–æ URL: {len(all_urls)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
        unique_urls = set(all_urls)
        print(f"üîó –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL: {len(unique_urls)}")
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        url_counts = Counter(all_urls)
        duplicates = {url: count for url, count in url_counts.items() if count > 1}
        
        print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê –î–£–ë–õ–ò–ö–ê–¢–û–í:")
        print("="*40)
        print(f"üìà –í—Å–µ–≥–æ URL: {len(all_urls)}")
        print(f"üîó –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL: {len(unique_urls)}")
        print(f"üîÑ –î—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(all_urls) - len(unique_urls)}")
        print(f"üìã URL —Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏: {len(duplicates)}")
        
        if duplicates:
            print(f"\nüîÑ –¢–û–ü-10 URL –° –ù–ê–ò–ë–û–õ–¨–®–ò–ú –ö–û–õ–ò–ß–ï–°–¢–í–û–ú –î–£–ë–õ–ò–ö–ê–¢–û–í:")
            print("-" * 50)
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            sorted_duplicates = sorted(duplicates.items(), key=lambda x: x[1], reverse=True)
            
            for i, (url, count) in enumerate(sorted_duplicates[:10]):
                print(f"\n{i+1}. üîó URL –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è {count} —Ä–∞–∑:")
                print(f"   {url[:100]}...")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥—É–±–ª–∏–∫–∞—Ç–∞—Ö
                duplicate_items = [item for item in data if item['image_url'] == url]
                print(f"   üìù –í –ø–æ—Å—Ç–∞—Ö:")
                for item in duplicate_items:
                    print(f"      ‚Ä¢ {item['shortCode']} ({item['image_type']}) - {item['timestamp']}")
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ —Ç–∏–ø–∞–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        print(f"\nüìä –ê–ù–ê–õ–ò–ó –î–£–ë–õ–ò–ö–ê–¢–û–í –ü–û –¢–ò–ü–ê–ú –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô:")
        print("-" * 45)
        
        type_analysis = {}
        for item in data:
            img_type = item['image_type']
            if img_type not in type_analysis:
                type_analysis[img_type] = {'total': 0, 'unique': 0, 'duplicates': 0}
            
            type_analysis[img_type]['total'] += 1
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ URL –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
        for img_type in type_analysis:
            type_urls = [item['image_url'] for item in data if item['image_type'] == img_type]
            unique_type_urls = set(type_urls)
            type_analysis[img_type]['unique'] = len(unique_type_urls)
            type_analysis[img_type]['duplicates'] = len(type_urls) - len(unique_type_urls)
        
        for img_type, stats in sorted(type_analysis.items()):
            print(f"   {img_type}:")
            print(f"      üìà –í—Å–µ–≥–æ: {stats['total']}")
            print(f"      üîó –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {stats['unique']}")
            print(f"      üîÑ –î—É–±–ª–∏–∫–∞—Ç–æ–≤: {stats['duplicates']}")
            if stats['duplicates'] > 0:
                duplicate_rate = (stats['duplicates'] / stats['total']) * 100
                print(f"      üìä –ü—Ä–æ—Ü–µ–Ω—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicate_rate:.1f}%")
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –ø–æ—Å—Ç–∞–º
        print(f"\nüìä –ê–ù–ê–õ–ò–ó –î–£–ë–õ–ò–ö–ê–¢–û–í –ü–û –ü–û–°–¢–ê–ú:")
        print("-" * 35)
        
        post_analysis = {}
        for item in data:
            post_code = item['shortCode']
            if post_code not in post_analysis:
                post_analysis[post_code] = {'total': 0, 'unique': 0, 'duplicates': 0}
            
            post_analysis[post_code]['total'] += 1
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ URL –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Å—Ç–∞
        for post_code in post_analysis:
            post_urls = [item['image_url'] for item in data if item['shortCode'] == post_code]
            unique_post_urls = set(post_urls)
            post_analysis[post_code]['unique'] = len(unique_post_urls)
            post_analysis[post_code]['duplicates'] = len(post_urls) - len(unique_post_urls)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å—Ç—ã —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        posts_with_duplicates = {post: stats for post, stats in post_analysis.items() if stats['duplicates'] > 0}
        
        if posts_with_duplicates:
            print(f"üìã –ü–æ—Å—Ç—ã —Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏ ({len(posts_with_duplicates)} –∏–∑ {len(post_analysis)}):")
            
            sorted_posts = sorted(posts_with_duplicates.items(), key=lambda x: x[1]['duplicates'], reverse=True)
            
            for i, (post_code, stats) in enumerate(sorted_posts[:10]):
                print(f"   {i+1}. {post_code}: {stats['duplicates']} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑ {stats['total']} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        else:
            print("‚úÖ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ –ø–æ—Å—Ç–∞—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç
        report = {
            'summary': {
                'total_urls': len(all_urls),
                'unique_urls': len(unique_urls),
                'duplicates': len(all_urls) - len(unique_urls),
                'duplicate_urls': len(duplicates)
            },
            'duplicate_details': dict(sorted_duplicates[:20]),  # –¢–æ–ø-20 –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            'type_analysis': type_analysis,
            'post_analysis': {post: stats for post, stats in post_analysis.items() if stats['duplicates'] > 0}
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        import datetime
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        report_filename = f"duplicates_report_{timestamp}.json"
        
        with open(report_filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ –û—Ç—á–µ—Ç –æ –¥—É–±–ª–∏–∫–∞—Ç–∞—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_filename}")
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ URL
        unique_urls_list = list(unique_urls)
        unique_filename = f"unique_image_urls_{timestamp}.txt"
        
        with open(unique_filename, 'w', encoding='utf-8') as f:
            for url in unique_urls_list:
                f.write(f"{url}\n")
        
        print(f"üíæ –§–∞–π–ª —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ URL —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {unique_filename}")
        
        return {
            'total_urls': len(all_urls),
            'unique_urls': len(unique_urls),
            'duplicates': len(all_urls) - len(unique_urls),
            'duplicate_urls': len(duplicates)
        }
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return None

if __name__ == "__main__":
    result = check_duplicates()
    
    if result:
        print(f"\nüéâ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"üìä –ò—Ç–æ–≥–æ: {result['duplicates']} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑ {result['total_urls']} URL")
        
        if result['duplicates'] > 0:
            duplicate_percentage = (result['duplicates'] / result['total_urls']) * 100
            print(f"üìà –ü—Ä–æ—Ü–µ–Ω—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicate_percentage:.1f}%")
        else:
            print("‚úÖ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ!")
    else:
        print("\n‚ùå –ê–Ω–∞–ª–∏–∑ –Ω–µ —É–¥–∞–ª—Å—è")
