"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ–º–µ—Ç–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ MongoDB
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç perceptual hash –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ—Ö–æ–∂–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
"""

import os
import imagehash
from dotenv import load_dotenv
import pymongo
from collections import defaultdict
from tqdm import tqdm
from datetime import datetime

load_dotenv()
load_dotenv('mongodb_config.env')

def find_and_mark_duplicates(threshold: int = 5, dry_run: bool = False):
    """
    –ù–∞—Ö–æ–¥–∏—Ç –∏ –ø–æ–º–µ—á–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
    
    Args:
        threshold: –ü–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ Hamming distance (0-10)
                  5 = —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è (–Ω–∞—Ö–æ–¥–∏—Ç –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏)
        dry_run: –ï—Å–ª–∏ True, —Ç–æ–ª—å–∫–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ë–î
    """
    print("üîç –ü–û–ò–°–ö –ò –ü–û–ú–ï–¢–ö–ê –í–ò–ó–£–ê–õ–¨–ù–´–• –î–£–ë–õ–ò–ö–ê–¢–û–í")
    print("="*70)
    print(f"‚öôÔ∏è  Threshold: {threshold} (Hamming distance)")
    print(f"‚öôÔ∏è  Dry run: {'–î–∞ (—Ç–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞—Ç—å)' if dry_run else '–ù–µ—Ç (–ø–æ–º–µ—Ç–∏—Ç—å –≤ –ë–î)'}")
    print("="*70)
    
    # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ MongoDB
    mongodb_uri = os.getenv('MONGODB_URI', 'mongodb://trend_ai_user:LoGRomE2zJ0k0fuUhoTn@localhost:27017/instagram_gallery')
    client = pymongo.MongoClient(mongodb_uri)
    db = client["instagram_gallery"]
    collection = db["images"]
    
    print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ MongoDB —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å perceptual hash
    images_with_hash = list(collection.find({
        "image_hash": {"$exists": True, "$ne": None}
    }).sort("parsed_at", 1))  # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
    
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(images_with_hash)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å perceptual hash")
    
    if len(images_with_hash) == 0:
        print("‚ùå –ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å perceptual hash!")
        print("üí° –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python add_perceptual_hash_to_existing.py")
        return
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ—Ö–æ–∂–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    print("\nüîç –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
    duplicate_groups = []
    processed_ids = set()
    
    for i, img in enumerate(tqdm(images_with_hash, desc="–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")):
        if img["_id"] in processed_ids:
            continue
        
        try:
            current_hash = imagehash.hex_to_hash(img["image_hash"])
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ö–µ—à–∞ –¥–ª—è {img.get('post_id', 'N/A')}: {e}")
            continue
        
        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        similar_images = [img]  # –ü–µ—Ä–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ - "–æ—Ä–∏–≥–∏–Ω–∞–ª"
        processed_ids.add(img["_id"])
        
        for j, other_img in enumerate(images_with_hash[i+1:], start=i+1):
            if other_img["_id"] in processed_ids:
                continue
            
            try:
                other_hash = imagehash.hex_to_hash(other_img["image_hash"])
                distance = current_hash - other_hash
                
                if distance <= threshold:
                    similar_images.append(other_img)
                    processed_ids.add(other_img["_id"])
            except Exception as e:
                continue
        
        # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã, –¥–æ–±–∞–≤–ª—è–µ–º –≥—Ä—É–ø–ø—É
        if len(similar_images) > 1:
            duplicate_groups.append(similar_images)
    
    print(f"\nüìä –ù–∞–π–¥–µ–Ω–æ {len(duplicate_groups)} –≥—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
    
    if len(duplicate_groups) == 0:
        print("‚úÖ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ!")
        return
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    total_duplicates = sum(len(group) - 1 for group in duplicate_groups)
    print(f"üìä –í—Å–µ–≥–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {total_duplicates}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
    print(f"\nüìã –ü—Ä–∏–º–µ—Ä—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (–ø–µ—Ä–≤—ã–µ 5 –≥—Ä—É–ø–ø):")
    print("-" * 70)
    
    for i, group in enumerate(duplicate_groups[:5]):
        original = group[0]
        duplicates = group[1:]
        
        print(f"\nüîµ –ì—Ä—É–ø–ø–∞ {i+1}:")
        print(f"   –û–†–ò–ì–ò–ù–ê–õ:")
        print(f"      Post ID: {original.get('post_id', 'N/A')}")
        print(f"      Username: @{original.get('username', 'N/A')}")
        print(f"      Likes: {original.get('likes_count', 0)}")
        print(f"      Hash: {original.get('image_hash', 'N/A')}")
        print(f"   –î–£–ë–õ–ò–ö–ê–¢–´ ({len(duplicates)}):")
        
        for dup in duplicates:
            try:
                orig_hash = imagehash.hex_to_hash(original["image_hash"])
                dup_hash = imagehash.hex_to_hash(dup["image_hash"])
                distance = orig_hash - dup_hash
            except:
                distance = "?"
            
            print(f"      ‚Ä¢ Post ID: {dup.get('post_id', 'N/A')}, "
                  f"Username: @{dup.get('username', 'N/A')}, "
                  f"Distance: {distance}")
    
    if len(duplicate_groups) > 5:
        print(f"\n... –∏ –µ—â–µ {len(duplicate_groups) - 5} –≥—Ä—É–ø–ø")
    
    # –ü–æ–º–µ—á–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ –ë–î
    if not dry_run:
        print(f"\nüè∑Ô∏è  –ü–æ–º–µ—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ MongoDB...")
        marked_count = 0
        
        for group in tqdm(duplicate_groups, desc="–ü–æ–º–µ—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"):
            original = group[0]
            duplicates = group[1:]
            
            for dup in duplicates:
                try:
                    # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –∑–∞–ø–∏—Å–∏
                    orig_hash = imagehash.hex_to_hash(original["image_hash"])
                    dup_hash = imagehash.hex_to_hash(dup["image_hash"])
                    distance = int(orig_hash - dup_hash)
                except:
                    distance = None
                
                # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –¥—É–±–ª–∏–∫–∞—Ç
                collection.update_one(
                    {"_id": dup["_id"]},
                    {
                        "$set": {
                            "is_duplicate": True,
                            "duplicate_of": original["_id"],
                            "duplicate_of_post_id": original.get("post_id"),
                            "duplicate_hash_distance": distance,
                            "marked_duplicate_at": datetime.now().isoformat()
                        }
                    }
                )
                marked_count += 1
        
        print(f"‚úÖ –ü–æ–º–µ—á–µ–Ω–æ {marked_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ MongoDB")
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è is_duplicate
        print("üî® –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è is_duplicate...")
        collection.create_index("is_duplicate")
        print("‚úÖ –ò–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω!")
    else:
        print(f"\n‚ö†Ô∏è  DRY RUN MODE: –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ –ë–î –Ω–µ –≤–Ω–µ—Å–µ–Ω—ã")
        print(f"üí° –ó–∞–ø—É—Å—Ç–∏—Ç–µ –±–µ–∑ --dry-run –¥–ª—è –ø–æ–º–µ—Ç–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
    
    print(f"\n{'='*70}")
    print(f"‚úÖ –ó–ê–í–ï–†–®–ï–ù–û!")
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ –≥—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(duplicate_groups)}")
    print(f"üìä –í—Å–µ–≥–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {total_duplicates}")
    if not dry_run:
        print(f"‚úÖ –ü–æ–º–µ—á–µ–Ω–æ –≤ –ë–î: {marked_count}")
    print(f"{'='*70}")

def unmark_all_duplicates():
    """–°–Ω–∏–º–∞–µ—Ç –ø–æ–º–µ—Ç–∫—É –¥—É–±–ª–∏–∫–∞—Ç–∞ —Å–æ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    print("üîÑ –°–ù–Ø–¢–ò–ï –ü–û–ú–ï–¢–û–ö –î–£–ë–õ–ò–ö–ê–¢–û–í")
    print("="*70)
    
    mongodb_uri = os.getenv('MONGODB_URI', 'mongodb://trend_ai_user:LoGRomE2zJ0k0fuUhoTn@localhost:27017/instagram_gallery')
    client = pymongo.MongoClient(mongodb_uri)
    db = client["instagram_gallery"]
    collection = db["images"]
    
    print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ MongoDB —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–º–µ—á–µ–Ω–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    duplicates_count = collection.count_documents({"is_duplicate": True})
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ {duplicates_count} –ø–æ–º–µ—á–µ–Ω–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
    
    if duplicates_count == 0:
        print("‚úÖ –ù–µ—Ç –ø–æ–º–µ—á–µ–Ω–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤!")
        return
    
    # –°–Ω–∏–º–∞–µ–º –ø–æ–º–µ—Ç–∫–∏
    result = collection.update_many(
        {"is_duplicate": True},
        {
            "$set": {"is_duplicate": False},
            "$unset": {
                "duplicate_of": "",
                "duplicate_of_post_id": "",
                "duplicate_hash_distance": "",
                "marked_duplicate_at": ""
            }
        }
    )
    
    print(f"‚úÖ –°–Ω—è—Ç–æ –ø–æ–º–µ—Ç–æ–∫: {result.modified_count}")
    print("="*70)

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="–ü–æ–∏—Å–∫ –∏ –ø–æ–º–µ—Ç–∫–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
    parser.add_argument("--threshold", type=int, default=5, 
                       help="–ü–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ Hamming distance (0-10, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5)")
    parser.add_argument("--dry-run", action="store_true",
                       help="–¢–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ë–î")
    parser.add_argument("--unmark", action="store_true",
                       help="–°–Ω—è—Ç—å –ø–æ–º–µ—Ç–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å–æ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    args = parser.parse_args()
    
    if args.unmark:
        unmark_all_duplicates()
    else:
        find_and_mark_duplicates(threshold=args.threshold, dry_run=args.dry_run)

