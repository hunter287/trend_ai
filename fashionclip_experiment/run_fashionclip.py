#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ FashionCLIP
–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –æ–¥–µ–∂–¥—ã
"""

import json
import os
import sys
from PIL import Image
import requests
from io import BytesIO
import torch
from transformers import CLIPProcessor, CLIPModel
from tqdm import tqdm
from datetime import datetime

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
INPUT_FILE = 'data/sample_images.json'
OUTPUT_FILE = 'data/fashionclip_results.json'
MODEL_NAME = 'patrickjohncyh/fashion-clip'

# –°–ª–æ–≤–∞—Ä–∏ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ Ximilar –¥–∞–Ω–Ω—ã—Ö)
ATTRIBUTE_PROMPTS = {
    'categories': [
        'jacket', 'coat', 'dress', 'shirt', 'blouse', 'top', 'crop top',
        'pants', 'jeans', 'trousers', 'skirt', 'shorts',
        'sweater', 'cardigan', 'hoodie', 't-shirt',
        'accessories', 'bag', 'handbag', 'eyewear', 'sunglasses', 'hat'
    ],
    'colors': [
        'black', 'white', 'gray', 'grey', 'brown', 'beige', 'tan',
        'red', 'pink', 'orange', 'yellow', 'gold',
        'green', 'blue', 'navy', 'purple', 'violet',
        'multicolor', 'colorful', 'patterned'
    ],
    'materials': [
        'cotton', 'denim', 'leather', 'synthetic', 'polyester',
        'wool', 'silk', 'satin', 'velvet', 'lace',
        'knit', 'knitted', 'mesh', 'transparent', 'sheer',
        'suede', 'fur', 'fleece', 'nylon', 'textile'
    ],
    'styles': [
        'casual', 'formal', 'elegant', 'sporty', 'athletic',
        'vintage', 'retro', 'modern', 'classic', 'minimalist',
        'bohemian', 'boho', 'grunge', 'street style', 'preppy',
        'business', 'office', 'party', 'evening', 'summer', 'winter'
    ]
}

class FashionCLIPAnalyzer:
    def __init__(self, model_name=MODEL_NAME):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ FashionCLIP"""
        print(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")

        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"üñ•Ô∏è  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

        self.model = CLIPModel.from_pretrained(model_name).to(self.device)
        self.processor = CLIPProcessor.from_pretrained(model_name)

        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

    def load_image_from_url(self, url, timeout=10):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ URL"""
        try:
            response = requests.get(url, timeout=timeout)
            response.raise_for_status()
            image = Image.open(BytesIO(response.content)).convert('RGB')
            return image
        except Exception as e:
            print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return None

    def classify_attributes(self, image, attribute_type, candidates, top_k=5):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∞—Ç—Ä–∏–±—É—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if image is None:
            return []

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã
        text_prompts = [f"a photo of {candidate} clothing" for candidate in candidates]

        try:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ —Ç–µ–∫—Å—Ç
            inputs = self.processor(
                text=text_prompts,
                images=image,
                return_tensors="pt",
                padding=True
            ).to(self.device)

            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits_per_image = outputs.logits_per_image
                probs = logits_per_image.softmax(dim=1)[0]

            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            top_indices = probs.argsort(descending=True)[:top_k]

            results = []
            for idx in top_indices:
                results.append({
                    'name': candidates[idx.item()],
                    'confidence': float(probs[idx].item())
                })

            return results

        except Exception as e:
            print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ {attribute_type}: {e}")
            return []

    def analyze_image(self, image_url):
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        image = self.load_image_from_url(image_url)

        if image is None:
            return None

        results = {}

        # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–æ –∫–∞–∂–¥–æ–º—É —Ç–∏–ø—É –∞—Ç—Ä–∏–±—É—Ç–æ–≤
        for attr_type, candidates in ATTRIBUTE_PROMPTS.items():
            top_k = 5 if attr_type == 'colors' else 3
            results[attr_type] = self.classify_attributes(
                image, attr_type, candidates, top_k=top_k
            )

        return results

def process_dataset():
    """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç —á–µ—Ä–µ–∑ FashionCLIP"""
    print(f"\nüîç –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —á–µ—Ä–µ–∑ FashionCLIP\n")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ {INPUT_FILE}...")

    if not os.path.exists(INPUT_FILE):
        print(f"‚ùå –§–∞–π–ª {INPUT_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print(f"   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python prepare_dataset.py")
        sys.exit(1)

    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)

    samples = data['samples']
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(samples)} –æ–±—Ä–∞–∑—Ü–æ–≤")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    analyzer = FashionCLIPAnalyzer()

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    print(f"\nüé® –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...\n")

    processed_count = 0
    failed_count = 0

    for i, sample in enumerate(tqdm(samples, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞")):
        image_url = sample['image_url']

        try:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            fashionclip_results = analyzer.analyze_image(image_url)

            if fashionclip_results:
                sample['fashionclip_results'] = fashionclip_results
                processed_count += 1
            else:
                sample['fashionclip_results'] = {
                    'error': 'Failed to analyze image'
                }
                failed_count += 1

        except Exception as e:
            print(f"\n  ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–±—Ä–∞–∑—Ü–∞ {i+1}: {e}")
            sample['fashionclip_results'] = {
                'error': str(e)
            }
            failed_count += 1

    # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    data['metadata']['fashionclip_processed_at'] = datetime.now().isoformat()
    data['metadata']['fashionclip_processed_count'] = processed_count
    data['metadata']['fashionclip_failed_count'] = failed_count

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ {OUTPUT_FILE}...")

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(f"   –£—Å–ø–µ—à–Ω–æ: {processed_count}")
    print(f"   –û—à–∏–±–∫–∏: {failed_count}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    if processed_count > 0:
        print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ FashionCLIP:")

        all_categories = []
        all_colors = []
        all_materials = []
        all_styles = []

        for sample in samples:
            fc_results = sample.get('fashionclip_results', {})
            if 'error' not in fc_results:
                all_categories.extend([c['name'] for c in fc_results.get('categories', [])])
                all_colors.extend([c['name'] for c in fc_results.get('colors', [])])
                all_materials.extend([m['name'] for m in fc_results.get('materials', [])])
                all_styles.extend([s['name'] for s in fc_results.get('styles', [])])

        print(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(set(all_categories))}")
        print(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–≤–µ—Ç–æ–≤: {len(set(all_colors))}")
        print(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤: {len(set(all_materials))}")
        print(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∏–ª–µ–π: {len(set(all_styles))}")

        # –ü—Ä–∏–º–µ—Ä—ã —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if samples and samples[0].get('fashionclip_results'):
            print(f"\nüéØ –ü—Ä–∏–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:")
            fc = samples[0]['fashionclip_results']
            print(f"  –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {[c['name'] for c in fc.get('categories', [])[:3]]}")
            print(f"  –¶–≤–µ—Ç–∞: {[c['name'] for c in fc.get('colors', [])[:3]]}")
            print(f"  –ú–∞—Ç–µ—Ä–∏–∞–ª—ã: {[m['name'] for m in fc.get('materials', [])[:3]]}")
            print(f"  –°—Ç–∏–ª–∏: {[s['name'] for s in fc.get('styles', [])[:3]]}")

    print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è HTML –æ—Ç—á—ë—Ç–∞")

if __name__ == '__main__':
    process_dataset()
